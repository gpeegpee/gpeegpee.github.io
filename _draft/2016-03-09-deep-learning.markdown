# Deep Learning
[TOC]

## Keyword
- [ ] Activation function
- [ ] Perceptron
- [ ] Multi layer perceptron(MLP)
- [ ] Regression, Classification, Clustering
- [ ] Regularization
- [ ] Backpropagation
- [ ] Supervised Learning, Unsupervised Learning, Reinforcement Learning

## Algorithms
- [ ] Adaboost
- [ ] Support Vector Maching(SVM)
- [ ] Convolutional Neural Network(CNN)
- [ ] Recurrent Neural Network(RNN)
- [ ] Long short-term memory(LSTM)
- [ ] Deep Belief Network(DBN)
- [ ] Restricted Boltzmann Machine(RBM)

## Optimization
- [ ] Batch normalization
- [ ] Weight normalization
- [ ] Whitening

* * *
![](https://standardfrancis.files.wordpress.com/2015/04/screenshot-from-2015-04-16-133436.png?w=1008)
$$
\mu\beta \leftarrow \frac{1}{m} \sum_{i=1}^{m}{x_i}  // mini\, batch \, mean \\
\sigma^2\beta \leftarrow \frac{1}{m} \sum_{i=1}^{m}{(x_i - \mu\beta)^2}  // mini-batch\,variance \\

\\
^3/_7\\

\sqrt{\frac{a}{b}}\\

\sqrt[n]{1+x+x^2+x^3+\ldots} \\

\sum_{i=1}^{10} t_i \\

\left(\right)\\

\left(\frac{x^2}{y^3}\right) \\

50 \textrm{ apples} \times 100 \textbf{ apples} = \textit{lots of apples}^2 \\

x = \frac{-b \pm \sqrt{b^2-4ac}}{2a} \\
\delta \alpha \beta \gamma \epsilon \eta \tau \varepsilon \zeta \nu \omega \varphi \phi \pi
\upsilon \theta \vartheta \iota \varpi \kappa \rho \lambda \Delta \Sigma \mu \cdot f(x)\\

\widehat a \hat a  \equiv \triangleleft 
$$

$$
\widehat a \hat a  \equiv \triangleleft
$$


```mermaid
sequenceDiagram
	Alice->Bob: Hello Bob, how are you?
	Note right of Bob: Bob thinks
	Bob-->Alice: I am good thanks!
	Bob-->John the Long: How about you John?
	Bob-->Alice: Checking with John...
	Alice->John the Long: Yes... John, how are you?
	John the Long-->Alice: Better then you!
```

```mermaid
graph LR;
    A[Hard edge]-->B(Round edge);
    B-->C{Decision};
    C-->D[Result one];
    C-->E[Result two];
    style A fill:#FF0000;
    style B fill:#00FF00;
    style C fill:#0000FF;
    style D fill:#FFF000;
    style E fill:#0FFFF0;
```



